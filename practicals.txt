
										MangoDB
										=======
		
Down load and install MongoDB
Goto path Server\3.0\bin> and run mangod.exe
in new Cmd > run mango.exe

Configure env.:-
	put pat till bin. ie Server\3.0\bin

---
working with MoangoDB
start Mangodb > mango.exe

	function times2(num){
	return num*2;
	}
> times2(5);
output :- -

Get a list of dbs --> show dbs
get a current db --> db
use db --> use database_name 
creating tables--> emp ={"empid ":"et-01",
			"ename ":"sahcin",
			"sal": "1000"
			"hiredate" : new Date()}


output--

now to insert into my database --> db.database_name.insert(table_name)
				ie. db.test_1.insert(emp)


locate tables details in db --> db.test_1.find()
output :-

to get a table details from db in good format-> db.test_1.find().pretty()



update table ---> db.test_1.update({"ename":"sachin"}, vibhav as)
remove details from tabel --> db.test_1.remove({ename":"sachin"})

js executing inside Monagiodb--> create a JS script and store in desktop
		load('selectDatabase.js')
		selectDB();

Linix store a mangorc.js file inside a user diretory 


==============================================================================================
What is use of hcatalog in hadoop?
HCatalog supports reading and writing files in any format for which a Hive SerDe (serializer-deserializer) can be written. By default, HCatalog supports RCFile, CSV, JSON, and SequenceFile formats. To use a custom format, you must provide the InputFormat, OutputFormat, and SerDe.

HCatalog is built on top of the Hive metastore and incorporates components from the Hive DDL. HCatalog provides read and write interfaces for Pig and MapReduce and uses Hive’s command line interface for issuing data definition and metadata exploration commands. It also presents a REST interface to allow external tools access to Hive DDL (Data Definition Language) operations, such as “create table” and “describe table”.

HCatalog presents a relational view of data. Data is stored in tables and these tables can be placed into databases. Tables can also be partitioned on one or more keys. For a given value of a key (or set of keys) there will be one partition that contains all rows with that value (or set of values).


Here is a very basic example of how ho use HCATALOG.

I have a table in hive ,TABLE NAME is STUDENT which is stored in one of the HDFS location:

neethu  90
malini  90
sunitha 98
mrinal  56
ravi    90
joshua  8

Now suppose I want to load this table to pig for further transformation of data, In this scenario I can use HCATALOG:

When using table information from the Hive metastore with Pig, add the -useHCatalog option when invoking pig:

pig -useHCatalog

(you may want to export HCAT_HOME 'HCAT_HOME=/usr/lib/hive-hcatalog/')

Now loading this table to pig: A = LOAD 'student' USING org.apache.hcatalog.pig.HCatLoader();

Now you have loaded the table to pig.To check the schema , just do a DESCRIBE on the relation.

DESCRIBE A


-------------------------------------------
Static Partitioning—Used when the values for partition columns are known well in advance of loading the data into a Hive table
		Static Partition - Using individual insert:-
			INSERT INTO TABLE Unm_Parti PARTITION(department='A') 
			SELECT EmployeeID, FirstName,Designation,Salary FROM Unm_Dup_Parti WHERE department='A'; 

			INSERT INTO TABLE Unm_Parti PARTITION (department='B') 
			SELECT EmployeeID, FirstName,Designation,Salary FROM Unm_Dup_Parti WHERE department='B'; 

	If we go for the above approach , if we have 50 partitions we need to do the insert statement 50 times. That is a tedeous task and it is known as Static Partition.
Dynamic Partitioning—Used when the values for partition columns are known only during loading of the data into a Hive table  
		Inorder to achieve the same we need to set 4 things,:-
		1. set hive.exec.dynamic.partition=true
			 This enable dynamic partitions, by default it is false.
		2. set hive.exec.dynamic.partition.mode=nonstrict
			 We are using the dynamic partition without a static partition (A table can be partitioned based on multiple columns in hive) in such case we have to enable the non strict mode. In strict mode we can use dynamic partition  only with a Static Partition.
		3. set hive.exec.max.dynamic.partitions.pernode=3
			 The default value is 100, we have to modify the same according to the possible no of partitions
		4. hive.exec.max.created.files=150000
			 The default values is 100000 but for larger tables it can exceed the default, so we may have to update the same.            
		INSERT OVERWRITE TABLE Unm_Parti PARTITION(department) SELECT EmployeeID, FirstName,Designation,Salary,department FROM Unm_Dup_Parti; 


Sqoop importing table from mysql to Hive

		Naviagte to mysql > Database > Table 

		On anther window , Naviagte to Hive 

		Anther window shell , Open new terminal 
			sqoop import --connect jdbc:mysql://localhost/test --table x1 -m 1 --hive-import

			sqoop import --connect jdbc:DatabaseType://(mysql is configure in localhost)/(databaseName) --table (tableName which you want to import) -m (number of mappers) --(if want ot import with complete Structure of table )




			
OOize , What is use?
	Tasks performed in Hadoop sometimes require multiple Map/Reduce jobs to be chained together to complete its goal. Within the Hadoop ecosystem, there is a relatively new component Oozie, which allows one to combine multiple Map/Reduce jobs into a logical unit of work, accomplishing the larger task.

Oozie workflow is a collection of actions (i.e. Hadoop Map/Reduce jobs, Pig jobs) arranged in a control dependency DAG (Direct Acyclic Graph), specifying a sequence of actions execution. This graph is specified in hPDL (a XML Process Definition Language).


Oozie is having control node (define from where the job should be strated ) and Action node(number of jobs equal to number of Action node). This configuration is define in "workflow.xml"file 

How to start Oozie service --> Bootsstrap
command to start Bootsstrapservice	sudo /etc/init.d/Oozie start 
 

 home/hadoop/hadoop/conf   > ls -alrt
 


MapReduce Joins:->  3 types of joins, Reduce-Side joins, Map-Side joins and the Memory-Backed Join

Note:- if two dataset both are large dataset then go for Reduce-side join 
If out of two dataset one is larger and anther is smaller then go for Map-side join 

Map-side join:- take small section of file and add file into distributed caches a pass to all distribute system 


How to add text file to ORC table in hive using mapreduce 
	1. 	CREATE EXTERNAL TABLE IF NOT EXISTS Cars(
			STORED AS TEXTFILE
			location '/user/<username>/visdata';

	2. Create the ORC table. :- CREATE TABLE IF NOT EXISTS mycars()
									FIELDS TERMINATED BY ','
									STORED AS ORC;
	3. Insert the data from the external table to the Hive ORC table. 
			INSERT OVERWRITE TABLE mycars SELECT * FROM cars;
	4. Verify that you imported the data into the ORC-formatted table correctly:
			 select * from mycars limit 3;
			 
Using mapreduce --> http://hadoopcraft.blogspot.in/2014/07/generating-orc-files-using-mapreduce.html



File systems in HDFS:-
---------------------

Avro File system :-
	1. Data and schema is stored together.(schema is attached with data ) 
	2. Data is stored in binary formate(Means taking very small size)
	3. Arvo file system is  used for "Data serialization"  -->(Data Serialization -> means storing data and reading back And "Data Exchange"--> Transfering message from one system to anther )
	4. Lets say we stored data in HDFS using java language is serialization and read data with C++ language is Deserialization
	
	
	
		ORC file formate and their supporting compression technice :
		----------------------------------------------------------
Must Read links on Data File system :- https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_performance_tuning/content/hive_perf_best_pract_use_orc_file_format.html
http://www.inquidia.com/news-and-info/hadoop-file-formats-its-not-just-csv-anymore

How data store in ORC :-   ORC stores collections of rows in one file and within the collection the row data is stored in a columnar format. This allows parallel processing of row collections across a cluster. Each file with the columnar layout is optimised for compression and skipping of data/columns to reduce read and decompression load.

ORC introduces a lightweight indexing that enables skipping of complete blocks of rows that do not match a query. It comes with basic statistics — min, max, sum, and count — on columns.

For example, a file may store rows 1–1,000 in the first group and row 1,001 to 2,000 in the next and both groups in one RCFile. The row group itself stores all the columns together. The first group then would save the first column of row 1 to 1,000 and then next column and so forth.

The benefit of grouping columns is a more efficient compression since similar data is near to each other. More importantly query conditions can be pushed down to read only relevant parts of a table. This is especially helpful with wide tables and queries that only apply to a few columns. In these cases Hive can skip large parts of the data to save IO and computing time.

Advantages With ORC File Format
	i) column stored separately
	ii) Stores statistics (Min,Max,Sum,Count)
	ii) Has Light weight Index
	iv) Larger Blocks by default 256 MB
	v) Reduce The accessing Time and storage Space 

	
Compression techniques :- 
ORC uses type specific readers and writers that provide light weight compression techniques such as 
		dictionary encoding, 
		bit packing, 
		delta encoding,
		and run length encoding -- resulting in dramatically smaller files. Additionally, ORC can apply generic compression using 
		zlib, 
		LZO, 
		or Snappy on top of the lightweight compression for even smaller files.
		 

TABLE to store ORC format:-
	CREATE  TABLE airanalytics (flightdate date ,uniquecarrier varchar(10),arrdelay int) 
	 PARTITIONED BY (flight_year String)
	 clustered BY (uniquecarrier)
	 sorted BY (flightdate)
	 INTO 24 buckets
	 stored AS orc tblproperties ("orc.compress"="NONE","orc.stripe.size"="67108864","orc.row.index.stride"="25000")

  Whereas :
		1. orc.compress indicates the compression techniques like NONE,Snappy,LZO etc
		2. orc.stripe.size indicates blocks size of file
		3. orc.row.index.stride indicates index
		
		
Sequence file :  is file consist of binary key/values pair
		Benefit of sequence file 
		1. To enable binary data as input formats
		2. To conpress small small files into one big file 
		
Note :- 1 file of 200 MB with 3 replication , how many pointers we need in the NN ? - > 6 pointers
Now if we have 200 files of 1 MB . how many pointers we need in NN ? -> 600 pointers
		
What kind of protocal suport to JDBC connection in Hive (more specificaly binary protocalls )
		1. http -->Connect to HiveServer2 using HTTP transport.
		2. binary --> Connect to HiveServer2 using TCP transport
				
						Sqoop
						-----
						
	What are the basic available commands in Hadoop sqoop ?
			Codegen 			 (Generate code to interact with database records)
			Create-hive-table    (Import a table definition into Hive)
			Eval				 (Evaluate a SQL statement and display the results)
			Export				 (Export an HDFS directory to a database table)
			Help				 (List available commands)
			Import				 (Import a table from a database to HDFS)
			Import-all-tables 	 (Import tables from a database to HDFS)
			List-databases		 (List available databases on a server)
			List-tables			 (List available tables in a database)
			Versions			 (Display version information)


						Interview quesgions on hive :-
						----------------------------
1.	CREATE TABLE mytable (name string,city string, employee_id int ) PARTITIONED BY (year STRING, month STRING, day STRING) CLUSTERED BY (employee_id) INTO 	256 	BUCKETS   
			How does the above store data in HDFS?
--> The data is loaded into HDFS and stored in files within directories. The schema is applied during Hive queries executions. The schema or metadata is read from the metastore via the HCatalog API

2. what is the difference between this metastore and the derby metastore also used by hive?  

3. How do you enfore a query to be a reduce side join?
			auto.reduce.join=true
			hive.map.join=false
			auto.convert.join=false

4. What are the main JVM processes running on a typical Hadoop worker node ?

5. I want to submit multiple jobs simultaneously to different queues and make sure there is a minimum capacity guarantee. Which scheduler should I opt for ?
			Default scheduler 
			Capacity Scheduler --correct
			Fair Scheduler
6.  Map 1: -/-    Map 2: -/- Map 1: 0/136    Map 2: 0/140 Map 1: 0(+14)/136    Map 2: 0/140 Map 1: 0(+24)/136    Map 2: 0/140 Map 1: 0(+34)/136    Map 2: 0/140  Status: Failed Vertex failed, vertexName=Map 2, vertexId=vertex_1454939795991_0012_1_01, diagnostics=[Task failed, taskId=task_1454939795991_0012_1_01_000026, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.OutOfMemoryError: Java heap space
		How do you fix this by setting hive parameters ?


7.  When there are 10 files each of 100Gb of equal priority. and the bandwidth is 1Gbps. What should be your best approach to transfer the file?
				Transfer file in one by one 
				OR ller
				group by it send these grouop ller


8.  Suppose there is a table that has to be repeatedly queried to find if empty or not , what are the best design that you would choose for the table. Note that the table would be always inserted or created using hive with always table stats updated
				Text file  
				create a ORC file--- correct

9. Suppose there is an ORC table tableA in hive that has to be repeatedly queried to find if empty or not , what are the best query that you would choose for the table. Note that the table would be always inserted or created using hive with always table stats updated.
				select count(*) from table A
				select count *) from (select 1 from table A)
				select count (*) from table A limit 1


10. A table consists of 3 columns colA, colB, colC . For a given combination of colA and colB  , if colC has more than one value , then you are supposed to display the multiple values of colC. 

for eg , for the below table 

ColA       ColB       ColC 
   1                  2               5 
   1                  2               4 
   2                  3               8 
   2                  3               8 
   2                  4               9 

You are only supposed to display  4 and 5 , because only that value of Col C has multiple values for same combination of ColA and Col B. 
If so, please choose the best solution.
				A.The query can be done with plain select query with a group by clause
				B.The query cannot be solved in a single plain query with a group by clause , it needs an inner query
				C.The query cannot be solved using SQL and requires some external code to parse the output and handle it


11. A table consists of 3 columns Col A, Col B, Col C. Find out the count of all null values in Col A , Col B and Col C. 
	For eg , for the below table 

Col A      Col B      Col C 
    1        2              5 
    1         NULL         4 
    2        3              8 
    2        3           NULL 
NULL         4           NULL 

The output should be 4 , because it has 4 null values in total. If so , please choose the best solution.
				A.The query can be solved with a single scan select query 
				B.The query will require 1 sub query 
				C.The query will require union of 3 sub query

12. How many rows of data as a block will be processed by the HIVE query engine if the below option is 
set : set hive.vectorized.execution.enabled = true;
				ADepending on the input block size
				BYou can set the block size as a parameter for each vectorized execution
				2^10 ---correct

13. The following statement failed to execute. What can be the cause? LOAD DATA LOCAL INPATH ‘${env:HOME}/country/state/’ OVERWRITE INTO TABLE address;
				A‘${env:HOME} does not follow HIVE syntax
				BPartition issues for the address table.
				CLocal inpath does not contain filename.--- correct 

14. What will be the result when you do cast(‘abc’ as INT)?
				A.Ascii values will be printed 
				B.Statement will error out 
				C.NULL
----------------------------------------------------------------------------------------------------------------------------
					Twitter Data Analyis with Hive      http://gethue.com/how-to-analyze-twitter-data-with-hue/
					--------------------------------
					JSON format process in hive :-->

Best site to get data --> https://catalog.data.gov/dataset?res_format=JSON  OR search for "json data set gov"

Editor to understand the format of json data  ---> http://jsonlint.com/

1. Dwonload Jar to process json format data in hive  --> http://files.cloudera.com/samples/hive-serdes-1.0-SNAPSHOT.jar
2. Store the same jar into lib folder of hive   OR add jar in class (always add jar before creating new table )
3. 



create external table tweet (
				retweeted boolean,
				id Bigint,
				timestamp_ms Bigint,
				created_at String, 
				favorite_count int, 
				text String, 
				retweeted_status Struct <
										retweeted:boolean, 
										extended_entities:Struct< 
																display_url:String>, 
										favorite_count:int,
										text:String, 
										entities:Struct< hashtags:Array<Struct<text:String>>, 	
														user_mentions:Array<Struct<screen_name:String>>
														>
										>)
Row format serde'com.cloudera.hive.serde.JSONSerDe'

location '/sachinG/Gain';  
//This is data location in HDFS. for simplification Naviate to DataNode in url and copy the location and paste here 

					

					Optimization of table :- 
					----------------------
 After uploading the data from such file into table. when data try to retrive from such table will take long time. AS it is frist deserialing the data and then retrive 

		So create anther table as imrror image of this table; with below query 

					create table new_table-log as select * from table-log;	
					
					
					
				XMl file format into Hive :-
				--------------------------
	create external table XMLTable1 (
		BusinessEntityID BigInt, 
		Name String,
		 SalePersonID BigInt,
		StoreSurvey map<String, String> )
		row format serde 'com.ibm.spss.hive.serde2.xml.XmlSerDe'
		with serdeproperties (
		"column.xpath.BusinessEntityID"="/Root/Store/@BusinessEntityID",	"column.xpath.Name"="/Root/Store/Name/",
		"column.xpath.SalePersonID"="/Root/Store/SalesPersonID/tex()",
		"column.xpath.StoreSurvey"="/Root/Store/Demographics/StoreSurvey/*") 
		stored as 
		Inputformat 'com.ibm.spss.hive.serde2.xml.XmlInputFormat' 
		Outputformat 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
		 Location "/sachinG/XML"
		tblproperties("xmlinput.start"="<Root","xmlinput.end"="</Root>");



					
Add Jar in Class path --> add jar /home/edureka/sachinG/hivexmlserde-1.0.5.3.jar   (this is a ABSOLUTE local path where jar is stored )




==========================================================================================================================
					Log-file format into hive --->https://archive.org/details/webserverlogs  (get of server logs)
					-------------------------
Get logs from site :- 
1. start hadoop and hive AND CREATE TABLE 
sample of log :--> 
log file contain IP, date, reuqaest, ul , http request, pot number , mugilo info , 

                       
CREATE EXTERNAL TABLE access(
  host STRING,
  identity STRING,
  user STRING,
  time STRING,
  request STRING,
  status STRING,
  size STRING,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\") ([^ \"]*|\"[^\"]*\"))?",
  "output.format.string" = "%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s"
)
STORED AS TEXTFILE
LOCATION '/logs/randomhacks.net/access';
					
					
Alter table emp add columns (details Struct<performace:int,Comments:String,promoted:boolean>);

How to Check the List of Jars Included in Hive ClassPath ?  --> hive> list jars;

A. Create Database
------------------
create database retail;
https://copy.com/Up4t3v3i94dM
B. Select Database
------------------
use retail;

C. Create table for storing transactional records
-------------------------------------------------
create table txnrecords(txnno INT, txndate STRING, custno INT, amount DOUBLE, 
category STRING, product STRING, city STRING, state STRING, spendby STRING)
row format delimited
fields terminated by ','
stored as textfile;

select * from txnrecords where category='Puzzles'; --> whether there will be a Map for this query? 

create external table externaltxnrecords(txnno INT, txndate STRING, custno INT, amount DOUBLE, 
category STRING, product STRING, city STRING, state STRING, spendby STRING)
row format delimited
fields terminated by ','
stored as textfile location '/hivetable';
[aditya is the folder which has all the data related to the table'

D. Load the data into the table [ From Linux client and not HDFS]
-------------------------------

LOAD DATA LOCAL INPATH '/home/edureka/sachin/txns1.txt' OVERWRITE INTO TABLE txnrecords;
E. Describing metadata or schema of the table
---------------------------------------------
describe txnrecords;

F. Counting no of records
-------------------------
select count(*) from txnrecords;

G. Counting total spending by category of products
--------------------------------------------------
select category, sum(amount) from txnrecords group by category;

H. 10 customers
--------------------
select custno, sum(amount) from txnrecords group by custno limit 10;

Table A --> 1000 rows and has a column called region and the values of the region [ usa,uk,india].

select col1,col2 from A where region='india';

I. Create partitioned table
---------------------------
create table txnrecsByCat(txnno INT, txndate STRING, custno INT, amount DOUBLE,
product STRING, city STRING, state STRING, spendby STRING)
partitioned by (category STRING)
clustered by (state) INTO 10 buckets
row format delimited
fields terminated by ','
stored as textfile;

Bucketing formula--> 
(hash of the bucketed column) % no of buckets = 

Venkat --> ca1ad34c030ecf7d3690f5cc531d2aad
Mukesh --> 71bf7cc5964bc4a79f7150d77603ad25

Table Customer -->	Col State [ 10 unique values ]
			Maharashtra, WestBengal, UttarPradesh, MadhyaPradesh, Kerala
			Col product [ 15 unique values ] 

	partition the data based on product col --> 15 partitions

	bucketed the data based on State col  in to 7 buckets --> 7 buckets

	a) Will all buckets have some value? 
	   Ans: May be as it is totally dependent on the value of the hash
	b) Will one bucket have more than 1 value? 
	   Ans: Yes
	c) Is there a guarantee that all similar values [ Maharashtra ]  will always go in one bucket.
	   Ans: Yes





J. Configure Hive to allow partitions
-------------------------------------

However, a query across all partitions could trigger an enormous MapReduce job if the table data and number of partitions are large. A highly suggested safety measure is putting Hive into strict mode, which prohibits queries of partitioned tables without a WHERE clause that filters on partitions. You can set the mode to nonstrict, as in the following session:

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.enforce.bucketing=true;

K. Load data into partition table
----------------------------------
from txnrecords txn INSERT OVERWRITE TABLE txnrecsByCat PARTITION(category)
select txn.txnno, txn.txndate,txn.custno, txn.amount,txn.product,txn.city,txn.state,
txn.spendby, txn.category DISTRIBUTE BY category;

Bucketing

SELECT txnno,product,state 
FROM txnrecsbycat TABLESAMPLE(BUCKET 3 OUT OF 10);

select txnno, product FROM txnrecsbycat TABLESAMPLE(BUCKET 2 OUT OF 10) order by txnno;


==========================
find sales based on age group
==========================

create table customer(custno string, firstname string, lastname string, age int,profession string)
row format delimited
fields terminated by ',';

load data local inpath '/home/cloudera/hive/custs' into table customer;

create table out1 (custno int,firstname string,age int,profession string,amount double,product string)
row format delimited                                                                                  
fields terminated by ',';   

insert overwrite table out1                                                                           
select a.custno,a.firstname,a.age,a.profession,b.amount,b.product                                     
from customer a JOIN txnrecords b ON a.custno = b.custno;     

select * from out1 limit 100;

create table out2 (custno int,firstname string,age int,profession string,amount double,product string, level string)
row format delimited                                                                                  
fields terminated by ',';   

insert overwrite table out2
select * , case
 when age<30 then 'low'
 when age>=30 and age < 50 then 'middle'
 when age>=50 then 'old' 
 else 'others'
end
from out1;


 select * from out2 limit 100; 

 describe out2;  

create table out3 (level string, amount double)                                                                                   
row format delimited
fields terminated by ',';

insert overwrite table out3  
 select level,sum(amount) from out2 group by level;


==============
simple join
==============

****emp.txt
****swetha,250000,Chennai
****anamika,200000,Kanyakumari
****tarun,300000,Pondi
****anita,250000,Selam


****email.txt
****swetha,swetha@gmail.com
****tarun,tarun@edureka.in
****nagesh,nagesh@yahoo.com
****venkatesh,venki@gmail.com


create table employee(name string, salary float,city string)
row format delimited
fields terminated by ',';

load data local inpath 'emp.txt' into table employee;

select * from employee where name='tarun';

create table mailid (name string, email string)
row format delimited
fields terminated by ',';


load data local inpath 'email.txt' into table mailid;

select a.name,a.city,a.salary,b.email from 
employee a join mailid b on a.name = b.name;

select a.name,a.city,a.salary,b.email from 
employee a left outer join mailid b on a.name = b.name;

select a.name,a.city,a.salary,b.email from 
employee a right outer join mailid b on a.name = b.name;

select a.name,a.city,a.salary,b.email from 
employee a full outer join mailid b on a.name = b.name;

===============================================
Custom Mapper Code to manipulate unix timestamp
===============================================

		CREATE TABLE u_data ( userid INT, movieid INT, rating INT, unixtime STRING) 
		ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE; 

		****1	101	8	1369721454
		****2	102	8	1369821454
		****3	103	8	1369921454
		****4	105	8	1370021454  
		****5	106	9	1370021454

		****And load it into the table that was just created:

		LOAD DATA LOCAL INPATH 'u.data' OVERWRITE INTO TABLE u_data; 

		Count the number of rows in table u_data:
		SELECT COUNT(*) FROM u_data; 

		****Create weekday_mapper.py:

		import sys 
		import datetime 
		for line in sys.stdin: 
			line = line.strip() 
			userid, movieid, rating, unixtime = line.split('\t') 
			weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() 
			print '\t'.join([userid, movieid, rating, str(weekday)]) 

		CREATE TABLE u_data_new ( 
			userid INT, 
			movieid INT, 
			rating INT, 
			weekday INT) 
		ROW FORMAT DELIMITED 
		FIELDS TERMINATED BY '\t'; 

		add FILE weekday_mapper.py; 

		****Note that columns will be transformed to string and delimited 
		****by TAB before feeding to the user script, and the standard output 
		****of the user script will be treated as TAB-separated string columns.

		****The following command uses the TRANSFORM clause to embed the mapper scripts.

		INSERT OVERWRITE TABLE u_data_new 
		SELECT 
			TRANSFORM (userid, movieid, rating, unixtime) 
			USING 'python weekday_mapper.py' 
			AS (userid, movieid, rating, weekday) 
		FROM u_data; 

		SELECT weekday, COUNT(*) 
		FROM u_data_new 
		GROUP BY weekday;


		===========
		UDF
		===========

		import java.util.Date;
		import java.text.DateFormat;
		import org.apache.hadoop.hive.ql.exec.UDF; 
		import org.apache.hadoop.io.Text;
		public class UnixtimeToDate extends UDF{
			public Text evaluate(Text text){
				if(text==null) return null;
				long timestamp = Long.parseLong(text.toString());
				return new Text(toDate(timestamp));
			}
			private String toDate(long timestamp) {
				Date date = new Date (timestamp*1000);
				return DateFormat.getInstance().format(date).toString();
			}
		}

		javac -classpath /usr/lib/hadoop-0.20/hadoop-core-0.20.2-cdh3u0.jar:/usr/lib/hive/lib/hive-exec-0.7.0-cdh3u0.jar UnixtimeToDate.java

		****Pack this class file into a jar: 
		$jar -cvf convert.jar UnixtimeToDate.class

		****Verify jar using command : 
		$jar -tvf convert.jar

		****add this jar in hive prompt
		ADD JAR  convert.jar;

		****Then you create your custom function as follows:
		create temporary function userdate as 'UnixtimeToDate';

		****one,1386023259550
		****two,1389523259550
		****three,1389523259550
		****four,1389523259550

		create table testing(id string,unixtime string)
		row format delimited
		fields terminated by ',';

		load data inpath '/data/counter' into table testing;

		hive> select * from testing;
		****OK
		****one		1386023259550
		****two		1389523259550
		****three	1389523259550
		****four	1389523259550

		****Then use function 'userdate' in sql command

		select id,userdate(unixtime) from testing;

		****OK
		****four	3/28/02 8:12 PM
		****one		4/30/91 1:59 PM
		****two		3/28/02 8:12 PM
		****three	3/28/02 8:12 PM

============================================================================
